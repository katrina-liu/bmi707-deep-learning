{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/katrina-liu/bmi707-deep-learning/blob/main/Week_2_Lab_Perceptron_by_hand.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZlKfUT1M3yHv"
      },
      "source": [
        "#Week 2 Lab: Perceptron by hand\n",
        "\n",
        "In this example, we will implement perceptron from scratch using only base pythong and numpy. \n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "########## Answer ##########\n",
        "# Adapted from http://scikit-learn.org/stable/auto_examples/model_selection/plot_learning_curve.html\n",
        "# Helper function\n",
        "from sklearn.metrics import roc_auc_score\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def plot_learning_curve(train_scores, title=None, ylim=None):\n",
        "    plt.figure()\n",
        "    plt.title(title)\n",
        "    if ylim is not None:\n",
        "        plt.ylim(*ylim)\n",
        "    plt.xlabel(\"Training Epochs\")\n",
        "    plt.ylabel(\"Loss Value\")\n",
        "    plt.grid()\n",
        "    plt.plot(range(len(train_scores)), train_scores, '-', color=\"r\",\n",
        "             label=\"Training score\")\n",
        "\n",
        "    plt.legend(loc=\"best\")\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "OTIzJvBQoFYD"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ex1YsYAr4zgT"
      },
      "source": [
        "## Question 1: Learning the OR function with a perceptron\n",
        "\n",
        "Here we will implement a perceptron from scratch. We will start with defining four data points as the input and build a perceptron to model them. The goal is to have the perceptron learn the __OR__ function."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dOQPyXcF3GhM"
      },
      "source": [
        "# import numpy and define the input data\n",
        "import numpy as np\n",
        "\n",
        "X = np.array([[0,0], [0,1], [1,0], [1,1]])\n",
        "y = np.array([0, 1, 1, 1]) "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fcDQ5jlM7w0P"
      },
      "source": [
        "##The forward and backwards pass\n",
        "\n",
        "Here we will implement a perceptron from scratch. We first need to compute the __forward pass__ of the perceptron, which is given by:\n",
        "\n",
        "\n",
        "$$\\phi(w_1*x_{1i} + w_2*x_{2i} + b)$$\n",
        "\n",
        "where $\\phi(h)$ is the the _sigmoid_ transformation and can be computed as:\n",
        "\n",
        "$$\\phi(h) = \\frac{1}{1+\\exp(-h)}$$\n",
        "\n",
        "In the example below, we will compute the forward pass for __all__ data points using a single matrix - vector multiplication. If weights $w_1, w_2$ are in a single vector $w$ we can multiply this vector with our $4x2$ $X$ matrix as follows:\n",
        "\n",
        "$$ h = Xw  $$\n",
        "\n",
        "We can then add our bias to $h$ and perform an element-wise sigmoid transformation to get a $4x1$ vector of output probabilities. \n",
        "\n",
        "Now, Recall that our gradients are for sample $i$ are given by:\n",
        "\n",
        "$$\\frac{\\partial{l}}{\\partial{w_{1}}} = (p - y)*p*(1-p)*x_{1i} \\\\\n",
        "\\frac{\\partial{l}}{\\partial{w_{2}}} = (p - y)*p*(1-p)*X_{2i} \\\\\n",
        "\\frac{\\partial{l}}{\\partial{b}} = (p - y)*p*(1-p)$$ \\\\\n",
        "\n",
        "First, we're going to set up the forward pass. This function should take in the `X` matrix, the weights `w`, and the bias term `b` and returns a vector of prediction probabilities `p` for each sample in `X`. \n",
        "\n",
        "Fill in the function template below:"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# The forward pass\n",
        "# X: an n x d matrix of the input features + a column of all ones for the intercept term\n",
        "# w: a d x 1 vector containing the weights for our peceptron\n",
        "def forward_pass(X, w, b):\n",
        "    p = 0 # your code for the forward pass goes here\n",
        "    return p"
      ],
      "metadata": {
        "id": "e3D9PjpHatma"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now the backward pass. The function should accept the label vector `y`, the data matrix `X`, and the vector of predicted probabilities `p`. Using the equations above, complete the backward pass function. "
      ],
      "metadata": {
        "id": "cgv7TpjbnNrX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# The backward pass\n",
        "# X: the design matrix of the input features\n",
        "# y: the ground truth of outcome labels (classes)\n",
        "# p: the class probability outputted by the model\n",
        "def backward_pass(y, X, p):\n",
        "    # Your code for the gradients goes below:\n",
        "    grad_w1 = 0\n",
        "    grad_w2 = 0\n",
        "    grad_b = 0\n",
        "    # Average the gradients\n",
        "    grad_w1 = grad_w1.mean()\n",
        "    grad_w2 = grad_w2.mean()\n",
        "    grad_b = grad_b.mean()\n",
        "    \n",
        "    return grad_w1, grad_w2, grad_b"
      ],
      "metadata": {
        "id": "i26Ruyl-eT-_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we are going to put it all together and come up with function that will train our perceptron. Please fill out the incomplete lines to complete the function"
      ],
      "metadata": {
        "id": "vUKEpsebpakN"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dhrKfsDboBlU"
      },
      "source": [
        "# One possible implementation is as below\n",
        "############################################################\n",
        "def train(X, y, w, b, iters, lr=1):\n",
        "    w_new = np.copy(w)\n",
        "    b_new = np.copy(b)\n",
        "    loss = []\n",
        "    for i in range(iters):\n",
        "        # preds is a 4x1 vector of probabilities\n",
        "        p = 0 # Call the forward pass function\n",
        "        # Now we want to com\n",
        "        grad_w1, grad_w2, grad_b = 0,0,0 # Call the backward pass function\n",
        "        \n",
        "        # Now do the gradient descent updates using the average gradient from all four samples\n",
        "        w_new[0] = 0 # Use the gradient descent update rule\n",
        "        w_new[1] = 0 # Use the gradient descent update rule\n",
        "        b_new = 0 # Use the gradient descent update rule\n",
        "\n",
        "        # Calculate the loss\n",
        "        mse = ((y - p)**2).mean()\n",
        "        loss.append(mse)\n",
        "        acc = len(np.where(np.round(p) == y)[0])/float(len(y))\n",
        "        # Print out some info \n",
        "        if i % 10 == 0:\n",
        "          print(\"Loss at iteration \" + str(i) + \": \" + str(np.mean(mse)) + '\\t' + \"Accuracy: \" + str(acc))\n",
        "    \n",
        "    return w_new, b_new, loss"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we are ready to train! Assuming we've done everything correctly the following should run and result in final values for the weights."
      ],
      "metadata": {
        "id": "xypJ4nYKqA4d"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RaUlx3Ch3w0y"
      },
      "source": [
        "# Initialize the weights and train the model\n",
        "w = np.array([1.,-1.])\n",
        "b = 0.\n",
        "w_new, w_b, loss = train(X, y, w, b, iters=100, lr=1)\n",
        "plot_learning_curve(loss)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Question 2: Learning the AND function\n",
        "\n",
        "Because we've written our functions in a modular way, we can easily repurose them to learn on new data. Next, we will train our perceptron to learn the AND function. Give the code a try again on the new data for the AND function."
      ],
      "metadata": {
        "id": "5Fa_ZaxpqeLx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# New data\n",
        "X = np.array([[0,0], [0,1], [1,0], [1,1]])\n",
        "y = np.array([0, 0, 0, 1]) \n",
        "\n",
        "# Reinitialize the weights and train the model\n",
        "w = np.array([1.,-1.])\n",
        "b = 0.\n",
        "w_new, w_b, loss = train(X, y, w, b, iters=100, lr=1)\n",
        "plot_learning_curve(loss)"
      ],
      "metadata": {
        "id": "VTvnczcNq3Qc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Question 3: (Trying to) Learn the XOR function\n",
        "\n",
        "Now we are going train our model on data for the XOR function. Run the code below and verify if it converges. If it doesn't, what do you think is the problem?"
      ],
      "metadata": {
        "id": "aofJkqQTrGrY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# New data\n",
        "X = np.array([[0,0], [0,1], [1,0], [1,1]])\n",
        "y = np.array([0, 1, 1, 0]) \n",
        "\n",
        "# Reinitialize the weights and train the model\n",
        "w = np.array([1.,-1.])\n",
        "b = 0.\n",
        "w_new, w_b, loss = train(X, y, w, b, iters=100, lr=1)\n",
        "plot_learning_curve(loss)"
      ],
      "metadata": {
        "id": "h6hpl5YNrV5I"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}